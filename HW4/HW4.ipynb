{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os \n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seed(config):\n",
    "    # 固定随机数种子\n",
    "    # 为了保证相同的网络跑出来的效果时完全相同的需要固定随机数种子\n",
    "    # 如果使用了pytorch等框架还需要确定对应的种子，以及cuda的种子\n",
    "    np.random.seed(config['seed'])\n",
    "    random.seed(config['seed'])\n",
    "    torch.manual_seed(config['seed'])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(config['seed'])\n",
    "        torch.cuda.manual_seed_all(config['seed'])\n",
    "    \n",
    "    # 在torch进行优化之前，会花费一段时间针对卷积搜索最适合的卷积实现算法，进而实现网络加速\n",
    "    # 当网络结构固定，输入结构固定时比较适宜使用\n",
    "    # 当网络结构或输入在不断调整变化时，每次都会重新搜索，这在模型优化时会浪费时间因此选择关闭\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # 保证每次卷积算法的输出是固定的，即默认算法，如果torch的种子是固定的，那么每次卷积的输入和输出也会是固定的\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping contain speaker2id and id2spekder\n",
    "# metadata contain two keys speakers and n_mels\n",
    "# speakers has some data like {'id': [{'feature_path': '*.pt', 'mel_len': 430}] ... } \n",
    "# the *.pt file shape is (mel_len, n_mels) \n",
    "# segment_len 限制了选取pt文件的长度，每次获得数据时随机选取其中的一段\n",
    "\n",
    "class myDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, segment_len=128):\n",
    "        self.data_dir = data_dir\n",
    "        self.segment_len = segment_len\n",
    "    \n",
    "        # Load the mapping from speaker neme to their corresponding id. \n",
    "        mapping_path = Path(data_dir) / \"mapping.json\"\n",
    "        mapping = json.load(mapping_path.open())\n",
    "        self.speaker2id = mapping[\"speaker2id\"]\n",
    "\t\n",
    "\t\t# Load metadata of training data.\n",
    "        metadata_path = Path(data_dir) / \"metadata.json\"    \n",
    "        metadata = json.load(open(metadata_path))[\"speakers\"]\n",
    "\n",
    "        # Get the total number of speaker.\n",
    "        self.speaker_num = len(metadata.keys())\n",
    "        self.data = []\n",
    "        for speaker in metadata.keys():\n",
    "            for utterances in metadata[speaker]:\n",
    "                self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ptPath, speakerID = self.data[idx]\n",
    "        try:\n",
    "            mel = torch.load(os.path.join(self.data_dir, ptPath))\n",
    "        except:\n",
    "            ptPath, speakerID = self.data[0]\n",
    "            mel = torch.load(os.path.join(self.data_dir, 'uttr-00a3bbf0dc02419c8ae56cdb72c2efa0.pt'))\n",
    "        \n",
    "        # Segmemt mel-spectrogram into \"segment_len\" frames.\n",
    "        if len(mel) > self.segment_len:\n",
    "            # Randomly get the starting point of the segment.\n",
    "            start = random.randint(0, len(mel) - self.segment_len)\n",
    "            # Get a segment with \"segment_len\" frames.\n",
    "            mel = torch.FloatTensor(mel[start:start+self.segment_len])\n",
    "        else:\n",
    "            mel = torch.FloatTensor(mel)\n",
    "\t\t# Turn the speaker id into long for computing loss later.\n",
    "        speaker = torch.FloatTensor([speakerID]).long()\n",
    "        return mel, speaker\n",
    "\n",
    "    def getSpeakerNum(self):\n",
    "        return self.speaker_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "\t# Process features within a batch.\n",
    "\t\"\"\"Collate a batch of data.\"\"\"\n",
    "\t# type of batch is list\n",
    "\t# batch contains batch_size tensor\n",
    "\tmel, speaker = zip(*batch)\n",
    "\t# Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n",
    "\tmel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n",
    "\t# mel: (batch size, length, 40)\n",
    "\treturn mel, torch.FloatTensor(speaker).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=80, n_spk=600, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Project the dimension of features from that of input into d_model\n",
    "        self.prenet = nn.Linear(40, d_model)\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, dim_feedforward=256, nhead=2\n",
    "        )\n",
    "\n",
    "        self.pred_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, n_spk)\n",
    "        )\n",
    "    \n",
    "    def forward(self, mels):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            mels: (batch size, length, 40)\n",
    "        return:\n",
    "            out: (batch size, n_spks)\n",
    "        \"\"\"\n",
    "        # out: (batch size, length, d_model)\n",
    "        out = self.prenet(mels)\n",
    "        # out: (length, batch size, d_model)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        # The encoder layer expect features in the shape of (length, batch size, d_model)\n",
    "        out = self.encoder_layer(out)\n",
    "        # out: (batch size, length, d_model)\n",
    "        out = out.transpose(0, 1)\n",
    "        # mean pooling\n",
    "        stats = out.mean(dim=1)\n",
    "\n",
    "        # out: (batch, n_spks)\n",
    "        out = self.pred_layer(stats)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### schedule learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer: Optimizer,\n",
    "    num_warmup_steps: int,\n",
    "    num_training_steps: int,\n",
    "    num_cycles: float = 0.5,\n",
    "    last_epoch: int = -1\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases following the values of the cosine function between the \n",
    "    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n",
    "    initial lr set in the optimizer\n",
    "\n",
    "    想办法让lr先增后减，这里使用装饰器来实现一个更加灵活的scheduler leraning rate\n",
    "\n",
    "    Args: \n",
    "        optimizer (: class: torch.optim.Optimizer):\n",
    "        The optimizer for which to schedule the learning rate\n",
    "        num_warmup_step (: obj: int):\n",
    "        The number of step for the warmup phase\n",
    "        num_training_step(: obj: int):\n",
    "        The total number of traing steps\n",
    "        num_cycles(:obj: float, optional, defaults to 0.5):\n",
    "        The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0 following a half-cosine).\n",
    "        last_epoch (: obj: int, optional, defaults to -1)\n",
    "        The index of the last epoch when resuming training\n",
    "\n",
    "    Return:\n",
    "        :obj: torch.optim.lr_scheduler.LambdaLR with the appropriate schedule.\n",
    "    \"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        # warmup\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        # decadence\n",
    "        progress = float(current_step - num_warmup_steps) / float(\n",
    "            max(1, num_training_steps - num_warmup_steps)\n",
    "        )\n",
    "        return max(\n",
    "            0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n",
    "        )\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Trainer(model, config, train_loader, valid_loader):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, config['num_warmup_steps'], config['num_epochs'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(config['device'])\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    best_acc = 0.0\n",
    "    stop_num = 0\n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_acc = [], []\n",
    "        for batch in tqdm(train_loader):\n",
    "            n_mel, labels = batch\n",
    "            n_mel, labels =  n_mel.to(config['device']), labels.to(config['device'])\n",
    "\n",
    "            outs = model(n_mel)\n",
    "            loss = criterion(outs, labels)\n",
    "\n",
    "            preds = outs.argmax(dim=1)\n",
    "            acc = torch.mean((preds==labels).float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 清除梯度\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_acc.append(acc.item())\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        acc0 = sum(train_acc) / len(train_acc)\n",
    "        loss0 = sum(train_loss) / len(train_loss)\n",
    "        scheduler.step()\n",
    "\n",
    "        writer.add_scalar(config['writerName'] + 'Loss/Train', loss0, epoch)\n",
    "        writer.add_scalar(config['writerName'] + 'Acc/Train', acc0, epoch)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        valid_loss, valid_acc = [], []\n",
    "        for batch in tqdm(valid_loader):\n",
    "            with torch.no_grad():\n",
    "                n_mels, labels = batch\n",
    "                n_mels, labels = n_mels.to(config['device']), labels.to(config['device'])\n",
    "\n",
    "                outs = model(n_mels)\n",
    "                loss = criterion(outs, labels)\n",
    "                preds = outs.argmax(dim=1)\n",
    "                acc = (preds == labels).float().mean()\n",
    "\n",
    "                valid_loss.append(loss.item())\n",
    "                valid_acc.append(acc.item())\n",
    "        \n",
    "        acc1 = sum(valid_acc) / len(valid_acc)\n",
    "        loss1 = sum(valid_loss) / len(valid_loss)\n",
    "\n",
    "        writer.add_scalar(config['writerName'] + 'Loss/Valid', loss1, epoch)\n",
    "        writer.add_scalar(config['writerName'] + 'Acc/Valid', acc1, epoch)\n",
    "\n",
    "        print(f\"epoch {epoch} / {config['num_epochs']}: [ train loss {loss0:.3f} | train acc {acc0:.3f} ]\")\n",
    "        print(f\"epoch {epoch} / {config['num_epochs']}: [ valid loss {loss1:.3f} | valid acc {acc1:.3f} ]\")\n",
    "\n",
    "        torch.save(model.state_dict(), config['last_model'])\n",
    "        if config['early_stop'] > stop_num:\n",
    "            if acc > best_acc:\n",
    "                stop_num = 0\n",
    "                print('Find a better model!!!')\n",
    "                torch.save(model.state_dict(), config['best_model'])\n",
    "            else:\n",
    "                stop_num += 1\n",
    "        else:\n",
    "            print('Cannot impove the model~')\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Train!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'segment_len': 128,\n",
    "    'valid_ratio': 0.1,\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 1e-4,\n",
    "    'gamma': 0.99,\n",
    "    'weight_decay': 1e-3,\n",
    "    'best_model': './model/best_model_1001.ckpt',\n",
    "    'last_model': './model/last_model_1001.ckpt',\n",
    "    'num_epochs': 700,\n",
    "    'early_stop': 50,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_warmup_steps': 10,\n",
    "    'writerName': 'HW4'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = myDataset('./data/', segment_len=config['segment_len'])\n",
    "setLen = len(dataset)\n",
    "validLen = int(setLen*config['valid_ratio'])\n",
    "train_set, valid_set = random_split(dataset, [setLen-validLen, validLen])\n",
    "train_loader = DataLoader(train_set, config['batch_size'], shuffle=True, drop_last=True, pin_memory=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid_set, config['batch_size'], drop_last=True, pin_memory=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer(model, config, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
